#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=============================================================================
Qwen2.5-0.5B GSM8K SFT Training Script using LLaMA-Factory
=============================================================================

ã€ä»£ç åŠŸèƒ½æ¦‚è¿°ã€‘
ä½¿ç”¨ LLaMA-Factory å¯¹ Qwen2.5-0.5B-Instruct åœ¨ GSM8K æ•°æ®é›†ä¸Šåšç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œ
é‡‡ç”¨ LoRA æŠ€æœ¯è¿›è¡Œé«˜æ•ˆå¾®è°ƒã€‚

ã€ä¸»è¦åŠŸèƒ½ã€‘
1. è‡ªåŠ¨å®‰è£…å’Œé…ç½® LLaMA-Factory
2. æ•°æ®é›†å‡†å¤‡å’Œæ ¼å¼è½¬æ¢
3. å¯åŠ¨ LoRA SFT è®­ç»ƒ
4. æ¨¡å‹åˆå¹¶å’Œä¿å­˜
5. è®­ç»ƒç›‘æ§å’Œæ—¥å¿—è®°å½•

ã€ä½¿ç”¨æ–¹æ³•ã€‘
1. ç¡®ä¿å·²å‡†å¤‡å¥½åŸºç¡€æ¨¡å‹å’Œæ•°æ®é›†
2. è¿è¡Œè„šæœ¬ï¼špython 02qwen05b_gsm8k_llamafactorysft.py
3. ç›‘æ§è®­ç»ƒï¼štensorboard --logdir=./logs

ã€ä¾èµ–ã€‘
pip install torch transformers datasets accelerate peft bitsandbytes tensorboard
git clone https://github.com/hiyouga/LLaMA-Factory.git

=============================================================================
"""

import os
import sys
import json
import logging
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Optional

# =============================================================================
# é…ç½®å‚æ•°
# =============================================================================

# é¡¹ç›®è·¯å¾„é…ç½®
PROJECT_ROOT = Path("/zk/code/llamafactorysft")
LLAMAFACTORY_DIR = PROJECT_ROOT / "LLaMA-Factory"
CONFIG_DIR = PROJECT_ROOT / "configs"
DATA_DIR = PROJECT_ROOT / "data"
SAVES_DIR = PROJECT_ROOT / "saves"
LOGS_DIR = PROJECT_ROOT / "logs"
MODELS_DIR = PROJECT_ROOT / "models"

# è®­ç»ƒé…ç½®
TRAIN_CONFIG_FILE = "qwen05b_lora_sft.yaml"
MERGE_CONFIG_FILE = "qwen05b_merge.yaml"

# æ¨¡å‹å’Œæ•°æ®è·¯å¾„
BASE_MODEL_PATH = "/zk/Qwen2.5-0.5B-Instruct"
TRAIN_DATA_PATH = DATA_DIR / "gsm8k_train_alpaca.json"
TEST_DATA_PATH = DATA_DIR / "gsm8k_test_alpaca.json"

# å…¨å±€ logger
logger: Optional[logging.Logger] = None

# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def setup_logging() -> logging.Logger:
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    log_file = LOGS_DIR / f"llamafactory_sft_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_file, encoding="utf-8"),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def run_command(cmd: str, cwd: Optional[Path] = None, check: bool = True) -> subprocess.CompletedProcess:
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    logger.info(f"Running command: {cmd}")
    if cwd:
        logger.info(f"Working directory: {cwd}")
    
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            cwd=cwd,
            capture_output=True,
            text=True,
            check=check
        )
        
        if result.stdout:
            logger.info(f"STDOUT: {result.stdout}")
        if result.stderr:
            logger.warning(f"STDERR: {result.stderr}")
            
        return result
    except subprocess.CalledProcessError as e:
        logger.error(f"Command failed with exit code {e.returncode}")
        logger.error(f"STDOUT: {e.stdout}")
        logger.error(f"STDERR: {e.stderr}")
        raise

def check_prerequisites():
    """æ£€æŸ¥å‰ç½®æ¡ä»¶"""
    logger.info("Checking prerequisites...")
    
    # æ£€æŸ¥åŸºç¡€æ¨¡å‹
    if not Path(BASE_MODEL_PATH).exists():
        raise FileNotFoundError(f"Base model not found: {BASE_MODEL_PATH}")
    logger.info(f"âœ“ Base model found: {BASE_MODEL_PATH}")
    
    # æ£€æŸ¥æ•°æ®é›†
    if not TRAIN_DATA_PATH.exists():
        raise FileNotFoundError(f"Training data not found: {TRAIN_DATA_PATH}")
    logger.info(f"âœ“ Training data found: {TRAIN_DATA_PATH}")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_path = CONFIG_DIR / TRAIN_CONFIG_FILE
    if not config_path.exists():
        raise FileNotFoundError(f"Training config not found: {config_path}")
    logger.info(f"âœ“ Training config found: {config_path}")

def install_llamafactory():
    """å®‰è£… LLaMA-Factory"""
    logger.info("Installing LLaMA-Factory...")
    
    if LLAMAFACTORY_DIR.exists():
        logger.info("LLaMA-Factory already exists, skipping installation")
        return
    
    # å…‹éš†ä»“åº“
    clone_cmd = f"git clone https://github.com/hiyouga/LLaMA-Factory.git {LLAMAFACTORY_DIR}"
    run_command(clone_cmd, cwd=PROJECT_ROOT)
    
    # å®‰è£…ä¾èµ–
    install_cmd = 'pip install -e ".[torch,metrics]"'
    run_command(install_cmd, cwd=LLAMAFACTORY_DIR)
    
    logger.info("âœ“ LLaMA-Factory installed successfully")

def prepare_llamafactory_config():
    """å‡†å¤‡ LLaMA-Factory é…ç½®"""
    logger.info("Preparing LLaMA-Factory configuration...")
    
    # å¤åˆ¶æ•°æ®é›†é…ç½®åˆ° LLaMA-Factory
    src_dataset_info = DATA_DIR / "dataset_info.json"
    dst_dataset_info = LLAMAFACTORY_DIR / "data" / "dataset_info.json"
    
    # è¯»å–ç°æœ‰é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    existing_config = {}
    if dst_dataset_info.exists():
        with open(dst_dataset_info, 'r', encoding='utf-8') as f:
            existing_config = json.load(f)
    
    # è¯»å–æˆ‘ä»¬çš„é…ç½®
    with open(src_dataset_info, 'r', encoding='utf-8') as f:
        our_config = json.load(f)
    
    # åˆå¹¶é…ç½®
    existing_config.update(our_config)
    
    # ä¿å­˜åˆå¹¶åçš„é…ç½®
    with open(dst_dataset_info, 'w', encoding='utf-8') as f:
        json.dump(existing_config, f, ensure_ascii=False, indent=2)
    
    # å¤åˆ¶æ•°æ®æ–‡ä»¶åˆ° LLaMA-Factory
    llamafactory_data_dir = LLAMAFACTORY_DIR / "data"
    llamafactory_data_dir.mkdir(exist_ok=True)
    
    import shutil
    shutil.copy2(TRAIN_DATA_PATH, llamafactory_data_dir / TRAIN_DATA_PATH.name)
    shutil.copy2(TEST_DATA_PATH, llamafactory_data_dir / TEST_DATA_PATH.name)
    
    logger.info("âœ“ LLaMA-Factory configuration prepared")

def start_training():
    """å¯åŠ¨è®­ç»ƒ"""
    logger.info("Starting SFT training...")
    
    config_path = CONFIG_DIR / TRAIN_CONFIG_FILE
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    train_cmd = f"llamafactory-cli train {config_path}"
    
    logger.info("=" * 60)
    logger.info("TRAINING STARTED")
    logger.info("=" * 60)
    logger.info(f"Config file: {config_path}")
    logger.info(f"Output directory: {SAVES_DIR / 'qwen05b_lora_sft'}")
    logger.info(f"Logs directory: {LOGS_DIR}")
    logger.info("=" * 60)
    
    start_time = datetime.now()
    
    try:
        # è¿è¡Œè®­ç»ƒï¼ˆä¸æ•è·è¾“å‡ºï¼Œè®©å…¶å®æ—¶æ˜¾ç¤ºï¼‰
        result = subprocess.run(
            train_cmd,
            shell=True,
            cwd=LLAMAFACTORY_DIR,
            check=True
        )
        
        end_time = datetime.now()
        training_duration = end_time - start_time
        
        logger.info("=" * 60)
        logger.info("TRAINING COMPLETED SUCCESSFULLY")
        logger.info("=" * 60)
        logger.info(f"Training duration: {training_duration}")
        logger.info(f"Model saved to: {SAVES_DIR / 'qwen05b_lora_sft'}")
        logger.info("=" * 60)
        
        return True
        
    except subprocess.CalledProcessError as e:
        logger.error(f"Training failed with exit code {e.returncode}")
        return False

def create_merge_config():
    """åˆ›å»ºæ¨¡å‹åˆå¹¶é…ç½®"""
    logger.info("Creating merge configuration...")
    
    merge_config = {
        "model_name_or_path": BASE_MODEL_PATH,
        "adapter_name_or_path": str(SAVES_DIR / "qwen05b_lora_sft"),
        "template": "qwen",
        "finetuning_type": "lora",
        "export_dir": str(MODELS_DIR / "qwen05b_gsm8k_merged"),
        "export_size": 2,
        "export_device": "cpu",
        "export_legacy_format": False
    }
    
    merge_config_path = CONFIG_DIR / MERGE_CONFIG_FILE
    
    # è½¬æ¢ä¸ºYAMLæ ¼å¼
    yaml_content = "\n".join([f"{k}: {v}" for k, v in merge_config.items()])
    
    with open(merge_config_path, 'w', encoding='utf-8') as f:
        f.write(yaml_content)
    
    logger.info(f"âœ“ Merge config created: {merge_config_path}")
    return merge_config_path

def merge_model():
    """åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹"""
    logger.info("Merging LoRA weights with base model...")
    
    merge_config_path = create_merge_config()
    
    # æ„å»ºåˆå¹¶å‘½ä»¤
    merge_cmd = f"llamafactory-cli export {merge_config_path}"
    
    try:
        run_command(merge_cmd, cwd=LLAMAFACTORY_DIR)
        logger.info(f"âœ“ Model merged successfully to: {MODELS_DIR / 'qwen05b_gsm8k_merged'}")
        return True
    except subprocess.CalledProcessError:
        logger.error("Model merging failed")
        return False

def print_summary():
    """æ‰“å°è®­ç»ƒæ€»ç»“"""
    logger.info("\n" + "=" * 80)
    logger.info("LLAMAFACTORY SFT TRAINING SUMMARY")
    logger.info("=" * 80)
    logger.info(f"ğŸ“ Project directory: {PROJECT_ROOT}")
    logger.info(f"ğŸ¤– Base model: {BASE_MODEL_PATH}")
    logger.info(f"ğŸ“Š Training data: {TRAIN_DATA_PATH}")
    logger.info(f"âš™ï¸  Training config: {CONFIG_DIR / TRAIN_CONFIG_FILE}")
    logger.info(f"ğŸ’¾ LoRA weights: {SAVES_DIR / 'qwen05b_lora_sft'}")
    logger.info(f"ğŸ”— Merged model: {MODELS_DIR / 'qwen05b_gsm8k_merged'}")
    logger.info(f"ğŸ“ˆ TensorBoard logs: {LOGS_DIR}")
    logger.info("")
    logger.info("ğŸš€ NEXT STEPS:")
    logger.info("   1. æ£€æŸ¥è®­ç»ƒæ—¥å¿—å’Œlossæ›²çº¿")
    logger.info("   2. ä½¿ç”¨WebUIæµ‹è¯•æ¨¡å‹æ•ˆæœ")
    logger.info("   3. è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½")
    logger.info("   4. æ ¹æ®æ•ˆæœè°ƒæ•´è¶…å‚æ•°é‡æ–°è®­ç»ƒ")
    logger.info("")
    logger.info("ğŸ“‹ USEFUL COMMANDS:")
    logger.info(f"   - TensorBoard: tensorboard --logdir={LOGS_DIR}")
    logger.info(f"   - WebUI: cd {LLAMAFACTORY_DIR} && llamafactory-cli webui")
    logger.info("=" * 80)

# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    """ä¸»å‡½æ•°"""
    global logger
    logger = setup_logging()
    
    logger.info("Starting Qwen2.5-0.5B GSM8K SFT Training with LLaMA-Factory")
    logger.info(f"Project root: {PROJECT_ROOT}")
    
    try:
        # 1. æ£€æŸ¥å‰ç½®æ¡ä»¶
        check_prerequisites()
        
        # 2. å®‰è£… LLaMA-Factory
        install_llamafactory()
        
        # 3. å‡†å¤‡é…ç½®
        prepare_llamafactory_config()
        
        # 4. å¯åŠ¨è®­ç»ƒ
        training_success = start_training()
        
        if training_success:
            # 5. åˆå¹¶æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            logger.info("\nDo you want to merge LoRA weights with base model? (y/n): ")
            # è‡ªåŠ¨åˆå¹¶ï¼Œç”Ÿäº§ç¯å¢ƒå¯ä»¥æ”¹ä¸ºäº¤äº’å¼
            merge_success = merge_model()
            
            if merge_success:
                logger.info("âœ… Training and merging completed successfully!")
            else:
                logger.warning("âš ï¸ Training completed but merging failed")
        else:
            logger.error("âŒ Training failed")
            return 1
        
        # 6. æ‰“å°æ€»ç»“
        print_summary()
        
        return 0
        
    except Exception as e:
        logger.error(f"Script failed with error: {str(e)}")
        logger.exception("Full traceback:")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)